# -*- coding: utf-8 -*-
"""Model_training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11DEt5EPprVJd4G_eKS6M5br4lZTgFiZG
"""

#model architecture inspired from https://pytorch.org/tutorials/beginner/data_loading_tutorial.html
from __future__ import print_function, division
import os
import torch
import torch.nn as nn
from torch.optim import Adam
from torch.utils.data import Dataset, DataLoader
import time
import re
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from skimage import io, transform
from torchvision import transforms, utils

# Ignore warnings
import warnings
warnings.filterwarnings("ignore")

plt.ion()   # interactive mode

class DNA_dataset(Dataset):
  def __init__(self, csv_file, dir):
    """
    Args:
      csv_file (string): path to the csv file with all pair-wise labels.
        1st line of the file has to be title because data at line 0 is skipped
        each line should be, 'sketch#, sketch#, boolean'
      dir (string): path to directory with all the DNA sequence pairs.
        each file should be a .txt or fasta file with a DNA sequence inside
      
    """
    self.path_DNA_sketches = dir 
    self.labels = pd.read_csv(csv_file) 
  
  def __len__(self):
    return len(self.labels)

  def __getitem__(self, idx):
    """
    get the 0-based idx-th element in the dataset
    returns a numpy array containing both sequences and the label for the pair of DNA sequences at that index

    DNA_dataset.DNA_pairs[0] = [ sketch1 = ACCTGTTACCC, sketch2 = GGTCAACACTTTA, label = False ]
    DNA_dataset.DNA_pairs[1] = [ sketch1 = AGTGACTGGAC, sketch2 = ACTGGACTCAACC, label = True ]
     .
     .
     .
    DNA_dataset.DNA_pairs[-1] = [ sketch1 = ACTGGATCAACC, sketch2 = AGTGACTGGACA, label = True ]
    """
    assert idx >= 0    
    line_of_interest = str( self.labels.iloc[idx] ).split() 
    #line of interest will look something like this :
    # ['title_line', 'True', 'Name:', '(3,', '1),', 'dtype:', 'object'] where we 
    # want to extract True, 3, and 1 as done using the re module below

    #creating the name of the sketch files we need access to and retrieving data
    sketch1_num = 'sketch_' + re.sub("[^0-9]", "", line_of_interest[3]) + '.txt'
    sketch2_num = 'sketch_' + re.sub("[^0-9]", "", line_of_interest[4]) + '.txt'
    with open(os.path.join(self.path_DNA_sketches,sketch1_num), 'r') as f:
      sketch1 = f.readlines()
    sketch1 = [float(re.sub("[^0-9]", "", x )) for x in sketch1[0].split()]
    with open(os.path.join(self.path_DNA_sketches, sketch2_num), 'r') as f:
      sketch2 = f.readlines()
    sketch2 = [float(re.sub("[^0-9]", "", x )) for x in sketch2[0].split()]
    input = np.array([ list(x) for x in zip(sketch1, sketch2)], dtype=np.float32)
    input = input.transpose((1,0))
    #encoding boolean into 0 and 1
    label = np.array([0], dtype=np.float32) if line_of_interest[1] == False else np.array([1], dtype=np.float32)
    #MAYBE YOU NEED TO DO THE RESHAPING MENTIONED IN FACE LANDMARK DATASET TUTORIAL BETWEEN NP AND TENSORS
    return ( torch.from_numpy(input), torch.from_numpy(label) )

#giving gooogle colab access to files in my drive i.e. dataset
from google.colab import drive
drive.mount('/content/drive')

#implementation of 1D convolutional net in torch
from torch.nn import Conv1d, Softmax, LeakyReLU, Linear, Flatten
class sketch_conv_model(nn.Module):
  def __init__(self):
    super(sketch_conv_model, self).__init__()
    self.conv1 = Conv1d(in_channels=1, out_channels=10, kernel_size=(2,4), stride=1)
    self.conv2 = Conv1d(in_channels=10, out_channels=5, kernel_size=(1,4), stride=1)
    self.conv3 = Conv1d(in_channels=5, out_channels=1,kernel_size=(1,4))
    self.nonlin1 = LeakyReLU()
    self.nonlin2 = LeakyReLU()
    #intuition behind the 1960 is that it should have one node per each entry in the output layer which is, i think,
    #( len(sketch) - kernel_size +1)* #output_channels = (200 - 4 + 1)*10 = 1970
    self.dense1 = Linear(191, 10)
    self.dense2 = Linear(10, 2)
    self.dense3 = Linear(2,1)
    self.flat = Flatten()
  
  def forward(self, x):
    x = self.conv1(x)
    x = self.conv2(x)
    x = self.nonlin1(x)
    x = self.conv3(x)
    x = self.nonlin2(x)
    x = self.dense1(x)
    x = self.dense2(x)
    x = self.dense3(x)
    return self.flat(x)
    # x = self.flat(x)
    # return nn.functional.softmax(x)
    # print("hello", x.shape)
    # return nn.functional.log_softmax(x, dim=None) #can try to make dim=None if error

#from https://www.pyimagesearch.com/2021/07/19/pytorch-training-your-first-convolutional-neural-network-cnn/
from torch.utils.data import random_split
from torchvision.transforms import ToTensor
from sklearn.preprocessing import OneHotEncoder

#dataset and model flag for when we had another model to test the architecture on with a different dataset
kmnist = False
#Data loading params
TRAIN_SPLIT = 0.8
VAL_SPLIT = 1.0 - TRAIN_SPLIT
batch_size = 64 #32


if kmnist:
  from torchvision.datasets import KMNIST
  trainData = KMNIST(root="data", train=True, download=True, transform=ToTensor())
  testData = KMNIST(root="data", train=False, download=True, transform=ToTensor())
else:
  #creating two instances of the dataset object defined above, one for training and one for
  trainData = DNA_dataset('/content/drive/MyDrive/train_dataset.csv', '/content/drive/MyDrive/sketches')
  testData = DNA_dataset('/content/drive/MyDrive/test_dataset.csv', '/content/drive/MyDrive/sketches')

print("tain_data length:", len(trainData))
print("test_data length:", len(testData))
# splitting the training dataset into a validation and training set so that our
# network can minimize validation loss between epochs
(trainData, valData) = random_split(trainData,[32000, 6097],
                                    generator=torch.Generator().manual_seed(42)) 

# creating train, validation, and test data loaders so that the data isn't loaded
# all at once into memory (theoretically)
trainDataLoader = DataLoader(trainData, shuffle=True, batch_size=batch_size)
valDataLoader = DataLoader(valData, batch_size=batch_size)
testDataLoader = DataLoader(testData, batch_size=batch_size)

#getting device that the model is run on

#defining model
model = sketch_conv_model()

#learning parameters
epochs = 2
loss_func = nn.BCEWithLogitsLoss() # nn.BCELoss()
optimzer = Adam(model.parameters())

trainSteps = len(trainDataLoader.dataset) // batch_size
valSteps = len(valDataLoader.dataset) // batch_size
#matrix to keep track of our stats across epochs
H = {
	"train_loss": [],
	"train_acc": [],
	"val_loss": [],
	"val_acc": []
}
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
startTime = time.time()

#TRAINING LOOP
for epoch in range(0, epochs):
  # set the model in training mode
  model.train()
  # initializing stat keeping variables
  totalTrainLoss = 0
  totalValLoss = 0
  trainCorrect = 0
  valCorrect = 0
  # loop over the training set
  for (x, y) in trainDataLoader:
    # send the input to the device
    (x, y) = (x.to(device), y.to(device))
    # perform a forward pass and calculate the training loss
    pred = model(x.unsqueeze(1))
    # print("pred=",pred.shape)
    # print(pred)
    # print("y=", y.shape)
    # print(y)
    #for NLLLoss i.e. LeNet model
    if kmnist:
      loss = loss_func(pred, y) 
    else:
      loss = loss_func(pred.float(), y.float())
    

    # zero out the gradients, perform the backpropagation step,
    # and update the weights
    optimzer.zero_grad()
    loss.backward()
    optimzer.step()
    # add the loss to the total training loss so far and
    # calculate the number of correct predictions
    totalTrainLoss += loss
    # print("pred.argmax(1)=", pred.argmax(1))
    # print("y=",y)
    trainCorrect += (pred.argmax(1) == torch.argmax(y)).type(torch.float).sum().item()

  #VALIDATION DATA EVALUATION
  with torch.no_grad():
    # set the model in evaluation mode
    model.eval()

    # loop over the validation set
    for (x, y) in valDataLoader:
      # send the input to the device
      (x, y) = (x.to(device), y.to(device))
      # make the predictions and calculate the validation loss
      pred = model(x.unsqueeze(1))
      totalValLoss += loss_func(pred.float(), y.float()) #y.unsqueeze(1)
      # calculate the number of correct predictions
      valCorrect += (pred.argmax(1) == y).type(torch.float).sum().item()
  
  #TRAINING STATISTICS

  # calculate the average training and validation loss
  avgTrainLoss = totalTrainLoss / trainSteps
  avgValLoss = totalValLoss / valSteps
  # calculate the training and validation accuracy
  trainCorrect = trainCorrect / len(trainDataLoader.dataset)
  valCorrect = valCorrect / len(valDataLoader.dataset)
  # update our training history
  H["train_loss"].append(avgTrainLoss.cpu().detach().numpy())
  H["train_acc"].append(trainCorrect)
  H["val_loss"].append(avgValLoss.cpu().detach().numpy())
  H["val_acc"].append(valCorrect)
  # print the model training and validation information
  print("[INFO] EPOCH: {}/{}".format(epoch + 1, epochs))
  print("Train loss: {:.6f}, Train accuracy: {:.4f}".format(avgTrainLoss, trainCorrect))
  print("Val loss: {:.6f}, Val accuracy: {:.4f}\n".format(avgValLoss, valCorrect))
  
endTime = time.time()
print("[INFO] total time taken to train the model: {:.2f}s".format(endTime - startTime))

#TEST DATASET EVALUATION
with torch.no_grad():
  # set the model in evaluation mode
  model.eval()
	
  # initialize a list to store our predictions
  preds = []
  # loop over the test set
  for (x, y) in testDataLoader:
    # send the input to the device
    x = x.to(device)
    print(x.size())
    print(x[None,].size())
    # make the predictions and add them to the list
    pred = model(x.unsqueeze(1))
    preds.extend(pred.argmax(axis=1).cpu().numpy())
  # generate a classification report
  print(classification_report(testData.targets.cpu().numpy(),
	np.array(preds), target_names=testData.classes))



# plot the training loss and accuracy
plt.style.use("ggplot")
plt.figure()
plt.plot(H["train_loss"], label="train_loss")
plt.plot(H["val_loss"], label="val_loss")
plt.plot(H["train_acc"], label="train_acc")
plt.plot(H["val_acc"], label="val_acc")
plt.title("Training Loss and Accuracy on Dataset")
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend(loc="lower left")
plt.savefig(args["plot"])

torch.save(model, '\content\drive\MyDrive\model.pt')
torch.load('\content\drive\MyDrive\model.pt')